{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unknowchar/NN-imgClasification/blob/main/NN_imgClasification_15052023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INTRODUCCIÓN\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ">## Dataset: MNIST\n",
        ">El set de datos elegido para este proyecto fue el data set de MNIST. Este dataset surgió alrededor de 1999 siendo un ejemplo básico y bueno para aquellos que deseen experimentar con las redes Neuronales y la computación visual, siendo un dataSet relativamente sencillo y común de entrenar entre los miles de datasets que existen en la actualidad.\n",
        ">### Características\n",
        "> * Está compuesto por 70,000 imágenes, de las cuales 60,000 de ellas son regularmente usadas para entrenar a la red neuronal y las otras 10,000 son usadas para realizar las pruebas de entrenamiento. \n",
        ">*Estás imágenes tienen un alto y ancho de 28px, cada una de ellas contiene un número del 0 al 9, siendo claramente 10 categorías para la salida de datos de está red. \n",
        ">* Todas estás imágenes están en una escala de grises.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ">## Arquitectura de red neuronal: Convolucional\n",
        ">>## Conceptos básicos\n",
        ">>### ¿Qué es la convolución?\n",
        "La convolución consiste en realizar un barrido de la imagen para tomar grupos de píxeles cercanos entre sí de la imagen de entrada y realizar un producto punto contra un kernel y un respectivo pooling.\n",
        "\n",
        ">>### Kernel\n",
        "El kernel es un filtro, el cual, se le aplica a la imagen a procesar para así extraer características especiales de la imagen para después hacer uso de estos nuevos valores filtrados. Principalmete se detectan bordes, desenfoque, entre otras características. Estos filtros están estructurados por matrices con valores predeterminados los cuales se multiplican contra un grupo de píxeles. <br> \n",
        "\n",
        ">>### Pooling\n",
        "El pooling tiene en común con el kernel el manejo de  agrupaciones de píxeles específicas, funciona también como un \"filtro\" tomando únicamente los valores más altos de esas agrupaciones especificadas hasta reducirlas notablemente para al final sear reagrupadas y mantener sus características, siendo posible combinarse junto con el  Kernel para obtener detalles más precisos.\n",
        "\n",
        ">### Red Neuronal Convolucional\n",
        ">### Estructura\n",
        "Las redes neuronales convolucionales suelen ser una arquitectura muy convencional para el procesamiento y clasificación de imágenes. Existen muchos modelos de redes neuronales convolucionales, pero al trabajar en los datos de MNIST hablaremos del modelo de RNC llamado LeNet. Este modelo está compuesto de dos partes, uno en donde hace recepción de los datos (su capa de entrada) mediante 2 capas convolucionales que incluyen su respectivo sistema de pooling.\n",
        "Las capas convolucionales se dedican a extraer características de las imágenes y de obetener una representación compacta y con base a esta seguir compactandola aun más y de realizar la calsificación de cada imagen.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        ">## Modelo Lenet\n",
        "El modelo Lenet utiliza la arquitectura de una red neuronal convolucional, disminuyendo su alto y su ancho pero al mismo tiempo de que este ocurre su profundidad aumenta progresivamente. \n",
        "<img src=\"https://www.codificandobits.com/img/posts/2019-04-26/red-lenet.png\">\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3J3h5YM1WG0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DESARROLLO"
      ],
      "metadata": {
        "id": "5RCaJDwUwluX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al utlizar el modelo LeNet necesitaremos múltiples secciones de keras para el desarrolllo de la red neuronal.\n",
        "\n",
        "### tensorflow.keras.datasets.mnist\n",
        "Como ya mencionamos anteriormente, nuestro dataset escogido fue el de mnist, el cual contiene miles de imágenes de números escritos a mano por diferentes personas, los cuales van del 0 hasta el 9.\n",
        "\n",
        "### tensorflow.keras.layers.Conv2D and .MaxPooling2D\n",
        "Ya que el modelo LeNet tiene la arquitectura de una RNC utilizaremos Conv2D para crear las capas convolucionales y el MaxPooling2D para realizar también el debido Pooling a las imágenes y poder filtrarlas.\n",
        "\n",
        "### tensorflow.keras.optimizers.SGD\n",
        "Con esta sección de keras incorporaremos el algoritmo de gradiente descendiente para proporcionar parte de los factores de aprendizaje necesarios.\n",
        "\n",
        "### tensorflow.keras.layers.Flatten and Dense\n",
        "Flatten nos ayudará a aplanar la imagén despues de que pase por las capas convolucionales y filtros del MaxPooling para después procesarlas mediante capas densas las cuales se encargarán de realizar las operaciones necesarias para su clasificación.\n",
        "\n",
        "### tensorflow.keras.utils.np_utils\n",
        "Esta herramienta nos ayudará a realizar una encocodificación one-hot para realizar la clasificación de las etiquetas, ya que Keras requiere de esta encodificación para las clasificaciones.\n",
        "\n",
        "### tensorflow.keras.models.Sequential\n",
        "Y por último importaremos la sección de Sequential la cual nos permitira crear el modelo (como su mismo nombre lo dice) secuencial de capas para el procesamiento y clasificación de las imágenes.\n",
        "\n"
      ],
      "metadata": {
        "id": "hiOKmKqrwoa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p8i8xev_50G4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as ts\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "También importaremos algunas herramientas proporcionadas por numpy.\n",
        "\n",
        "### np.random.seed(2)\n",
        "Utilizamos está semilla nos ayudará a que cada vez que la red neuronal entre en funcionamiento, los valores para los filtros y los coeficientes se reinicien en este mismo número proporcionado.\n",
        "\n",
        "### matplotlib.pyplot.plt\n",
        "Solo utilizaremos pyplot para realizar momentanamente una visualización de las imágenes a lo largo de nuestro entrenamiento, además de ayudarnos con el análisis de rendimiento para después realizar el tunning."
      ],
      "metadata": {
        "id": "EiFmDagL8_vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "np.random.seed(2)\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kAXRovMK8_fs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Carga y visualización de características de dataset\n",
        "\n",
        "Ahora carguemos el dataset.\n",
        "Para ello haremos uso de cuatro variables.\n",
        "\n",
        "\n",
        "*   train_X\n",
        "*   train_Y\n",
        "*   x_test\n",
        "*   y_test\n",
        "\n",
        "train_X contiene las imágenes mientras que train_Y contiene todas las etiquetas de estás imágenes.\n",
        "Mientras tanto las variables x_test e y_test, contienen estos mismo valores mencionados anteriormente, la única diferencia que hay entre estos dos es el contexto en el que serán utilizadas en la red neuronal.\n",
        "\n",
        "Recordemos que las redes neuronales necesitan de ejemplos para aprender a clasificar, es por eso que le damos una determinada cantidad de imágenes y etiquetas para ser utilizadas como referencia."
      ],
      "metadata": {
        "id": "aksBT5rX3VnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#CARGA DE DATASET PARA TRABAJAR SOBRE ÉL \n",
        "(train_X,train_Y),(x_test,y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "tO5Bcd0k6bPg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que se han cargado correctamente las imágenes junto con sus etiquetas.\n",
        "Siendo agregadas 60,000 imágenes para el entrenamiento y 10,000 para realizar pruebas durante el mismo, todas ellas con una resolución de 28x28px"
      ],
      "metadata": {
        "id": "GzE7ESLrAxUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Imágenes contenidas en train_X\")\n",
        "print(train_X.shape)\n",
        "\n",
        "print(\"Etiquetas para imágenes en train_Y\")\n",
        "print(train_Y.shape)\n",
        "\n",
        "print(\"Imágenes contenidas en x_test\")\n",
        "print(x_test.shape)\n",
        "\n",
        "print(\"Etiquetas para imágenes en y_test\")\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "Um51BKqJ7Ud4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a3d2b7-dfad-4d18-c21b-89df640d86a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imágenes contenidas en train_X\n",
            "(60000, 28, 28)\n",
            "Etiquetas para imágenes en train_Y\n",
            "(60000,)\n",
            "Imágenes contenidas en x_test\n",
            "(10000, 28, 28)\n",
            "Etiquetas para imágenes en y_test\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Ajuste de datos en imágenes y encodificación one-hot para etiquetas\n",
        "\n",
        "Debido a que las clasificaciones se realizar mediante 0 y 1 necesitamos normalizar los valores contenidos en las imágenes así que para ello dividiremos el valor real entre un valor estimado, en este caso 255 ya que los valores contenidos en la imagen tienen un rango de entre 0 a 255 por la escala RGB.\n",
        "\n",
        "En el caso de las etiquetas, keras hace uso de una encodificación llamado one hot, que da valores en bits a las etiquetas clasificandolas con 1 y 0 a lo largo de un array, siendo 0 la clasificación a la que no pertenece y 1 a la que sí.\n",
        "\n",
        "Para entender rápidamente esto, podemos tomar en cuenta la clasificación de tres colores: \n",
        "* Rojo \n",
        "* Verde\n",
        "* Azul\n",
        "\n",
        "Al utilizar la encodificación One-hot podríamos visualizarlo de la siguiente forma.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:837/1*d5-PQyRRjvzBZjI5f7X3hA.png\">\n",
        "\n",
        "Lo mismo se aplica para la clasifiación de los números al realizar nuestra encodificación hacia las etiquetas.\n",
        "\n",
        "Creamos una variable en donde especificaremos el número de clasificaciones, siendo estás un número de 10 y haremos uso de np_utils.categorical para la encodificación OH. Está función recibe dos parámetros, el array con las respectivas etiquetas y el número de clases, para realizar la encodificación.\n",
        "\n"
      ],
      "metadata": {
        "id": "l8yPGajhCQDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_X= train_X/255.0\n",
        "x_test= x_test/255.0\n",
        "\n",
        "\n",
        "train_Y= to_categorical(train_Y,10)\n",
        "y_test= to_categorical(y_test,10)\n"
      ],
      "metadata": {
        "id": "EujZ3rZ2B6Ew"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Visualización de imágenes\n",
        "\n",
        "Ahora con todas estás configuraciones realizadas hágamos la visualización de una imágen.\n"
      ],
      "metadata": {
        "id": "v1SwFkExGjcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nimagen = 100\n",
        "plt.imshow(train_X[nimagen,:].reshape(28,28), cmap='gray_r')\n",
        "plt.title(\"Imagen de ejemplo - Categoría: \" + str(np.argmax(train_Y[nimagen])))\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "6e5btIv_N0U3",
        "outputId": "6f912d51-3801-41c7-e09b-caa20d759a88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#Creación de modelo secuencial\n",
        "\n",
        "Ahora tomando en cuenta la vista en la introducción sobre la estructura del modelo LeNet\n",
        "<img src=\"https://www.codificandobits.com/img/posts/2019-04-26/red-lenet.png\">\n",
        "\n",
        "Podemos denotar las características de este modelo para cada una de las capas a crear. Antes de immplementarlo a código hagamos una lista de las capas que implementaremos\n",
        "\n",
        "\n",
        "# Listado de características de capas\n",
        "\n",
        "> CONV 1\n",
        "*   Filtros = 6 de 5x5x1\n",
        "*   padding=0\n",
        "*   strides=1\n",
        "*   Función de activación: ReLU\n",
        "\n",
        "> MAX-POOLING 1\n",
        "*   Filtros = 6 de 2x2\n",
        "*   padding=0\n",
        "*   strides=2\n",
        "*   Función de activación: Ninguna\n",
        "\n",
        "> CONV 2\n",
        "*   Filtros = 16 de 5x5x6\n",
        "*   padding=0\n",
        "*   strides=1\n",
        "*   Función de activación: ReLU\n",
        "\n",
        "> MAX-POOLING 2\n",
        "*   Filtros = 16 de 2x2\n",
        "*   padding=0\n",
        "*   strides=2\n",
        "*   Función de activación: Ninguna\n",
        "\n",
        "> Flatten\n",
        "*   No recibe características, solo aplanara los volúmenes proporcionados con anterioridad.\n",
        "\n",
        "\n",
        "> DENSE 1\n",
        "*   Neuronas = 120\n",
        "*   Función de activación: ReLU\n",
        "\n",
        "> DENSE 2\n",
        "*   Neuronas = 84 \n",
        "*   Función de activación: ReLU\n",
        "\n",
        "> SALIDA\n",
        "*   No de categorías = 10\n",
        "*   Función de activación: softmax\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LadTe8ceVqhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora primero necesitaremos crear el modelo secuencial, para ello necesitamos donde almacenarlo, así que utilizaremos la siguiente herramienta que sería crear un sequential, al cual llamaremos modelo. Este mismo contendrá todas las capas mencionadas con anterioridad y sus características."
      ],
      "metadata": {
        "id": "RoTnZoZ6YvPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Sequential()"
      ],
      "metadata": {
        "id": "FYF5Nhm6YTlu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después crearemos las capas."
      ],
      "metadata": {
        "id": "8O-mZP4XZDHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.add(Conv2D(filters=6,kernel_size=(5,5),activation=\"relu\",input_shape=(28,28,1)))\n",
        "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "modelo.add(Conv2D(filters=16,kernel_size=(5,5),activation=\"relu\"))\n",
        "modelo.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "modelo.add(Flatten())\n",
        "modelo.add(Dense(120,activation=\"relu\"))\n",
        "modelo.add(Dense(84, activation=\"relu\"))\n",
        "\n",
        "modelo.add(Dense(10,activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "6mb7QzTXZG-N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que cada una de las características mencionadas en la lista esta implementada en cada una de las capas agregadas al modelo convolucional.\n",
        "Los filtros en cada MaxPooling no se vuelven a mencionar por que se excepcionan a la hora de especificar la cantidad de filtros debido a que contendrían por default los de su capa anterior.\n",
        "En el caso del stride y el padding estos se declaran por default.\n",
        "\n",
        "---\n",
        "\n",
        "///Avance:\n",
        "Ivo Alberto Ramírez Gaeta\n",
        "15/05/2023\n",
        "4:02 p.m."
      ],
      "metadata": {
        "id": "B3Va-5ITaogC"
      }
    }
  ]
}